# -*- coding: utf-8 -*-
"""ProjectClassificator_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1omkn1cICZCTaekwXfR6Karla6VAmt1XD
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import matplotlib.pyplot as plt


BASE_PATH = "D:/Project/"

# Load text file paths
train_txt_path = os.path.join(BASE_PATH, "Category and Attribute Prediction Benchmark/Anno_fine/train.txt")
train_cate_path = os.path.join(BASE_PATH, "Category and Attribute Prediction Benchmark/Anno_fine/train_cate.txt")
train_attr_path = os.path.join(BASE_PATH, "Category and Attribute Prediction Benchmark/Anno_fine/train_attr.txt")

# Load image paths and ensure they are absolute paths
with open(train_txt_path, "r", encoding="utf-8") as f:
    image_paths = [os.path.abspath(os.path.join(BASE_PATH, line.strip())) for line in f.readlines()]

# Load category labels
with open(train_cate_path, "r", encoding="utf-8") as f:
    category_labels = np.array([int(line.strip()) for line in f.readlines()])

# Load attribute labels
with open(train_attr_path, "r", encoding="utf-8") as f:
    attribute_labels = np.array([[int(x) for x in line.split()] for line in f.readlines()])

# Ensure label consistency
assert len(image_paths) == len(category_labels) == len(attribute_labels), "Mismatch in data lengths"

# Convert category labels to zero-based indexing
unique_categories = sorted(set(category_labels))
category_to_index = {cat: idx for idx, cat in enumerate(unique_categories)}
category_labels = np.array([category_to_index[cat] for cat in category_labels])

num_categories = len(unique_categories)
num_attributes = attribute_labels.shape[1]

print(f"Unique Categories: {num_categories}, Attributes: {num_attributes}")

# Image size and batch size
IMG_SIZE = (300, 224)
BATCH_SIZE = 32

# Function to load and preprocess images
def load_and_preprocess_image(path):
    path = path.decode("utf-8")  # Ensure path is a valid string
    path = os.path.abspath(path)  # Convert to absolute path
    if not os.path.exists(path):
        print(f"ERROR: File not found: {path}")  # Print missing file
        return np.zeros((IMG_SIZE[0], IMG_SIZE[1], 3), dtype=np.float32)  # Return blank image to prevent crashing

    img = load_img(path, target_size=IMG_SIZE)  # Resize image
    img_array = img_to_array(img) / 255.0  # Normalize
    return img_array

# Function to create TensorFlow dataset (No Splitting)
def create_tf_dataset(image_paths, category_labels, attribute_labels, batch_size=32):
    def load_image_label(img_path, cat_label, attr_label):
        img = tf.numpy_function(load_and_preprocess_image, [img_path], tf.float32)
        img.set_shape((IMG_SIZE[0], IMG_SIZE[1], 3))  # Explicitly set shape
        return img, (cat_label, attr_label)

    dataset = tf.data.Dataset.from_tensor_slices((image_paths, category_labels, attribute_labels))
    dataset = dataset.map(load_image_label, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    
    return dataset

# Use the entire dataset for training (NO SPLITTING)
train_dataset = create_tf_dataset(image_paths, category_labels, attribute_labels, BATCH_SIZE)

# Load a Pretrained ResNet50 Model
base_model = ResNet50(weights="imagenet", include_top=False, input_shape=(300, 224, 3))
base_model.trainable = False  # Freeze base model for initial training

# Add new classification layers
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation="relu")(x)
x = Dropout(0.5)(x)

# Classification Head
category_output = Dense(num_categories, activation="softmax", name="category_output")(x)

# Attribute Head
attribute_output = Dense(num_attributes, activation="sigmoid", name="attribute_output")(x)

# Define and Compile Model with Learning Rate Decay
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=10000,
    decay_rate=0.9
)

model = Model(inputs=base_model.input, outputs=[category_output, attribute_output])
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
    loss={"category_output": "sparse_categorical_crossentropy", "attribute_output": "binary_crossentropy"},
    metrics={"category_output": "accuracy", "attribute_output": "accuracy"}
)

# Save Best Model
checkpoint_cb = tf.keras.callbacks.ModelCheckpoint("best_model.h5", save_best_only=True)

# Train model for 20 epochs (Initial Training)
history = model.fit(
    train_dataset,
    epochs=20,
    callbacks=[checkpoint_cb]
)

# Fine-Tuning: Unfreeze the top layers of ResNet50
base_model.trainable = True  # Unfreeze for fine-tuning

# Recompile with a lower learning rate for fine-tuning
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss={"category_output": "sparse_categorical_crossentropy", "attribute_output": "binary_crossentropy"},
    metrics={"category_output": "accuracy", "attribute_output": "accuracy"}
)

# Fine-Tune the Model for 30 More Epochs
history_finetune = model.fit(
    train_dataset,
    epochs=30,
    callbacks=[checkpoint_cb]
)

# Plot Loss
plt.xlabel('Epoch Number - (Iterations)')
plt.ylabel("Loss Magnitude")
plt.plot(history.history['loss'] + history_finetune.history['loss'])
plt.show()

# Prediction Function
def predict_image(model, image_path):
    """
    Loads an image, preprocesses it, and makes a prediction using the trained model.
    """
    image_path = os.path.abspath(image_path)  # Ensure absolute path
    IMG_SIZE = (300, 224)  # Ensure this matches the training size

    # Load and preprocess the image
    img = load_img(image_path, target_size=IMG_SIZE)
    img_array = img_to_array(img) / 255.0  # Normalize pixel values
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension

    # Make predictions
    category_pred, attr_pred = model.predict(img_array)

    # Get the predicted category index and attributes
    predicted_category = np.argmax(category_pred[0])  # Extract category index
    predicted_attributes = np.round(attr_pred[0])  # Convert probabilities to binary labels

    print(f"Predicted Category: {predicted_category}")
    print(f"Predicted Attributes: {predicted_attributes}")

    return predicted_category, predicted_attributes

# Example usage of prediction function
image_path = "D:/Project/Category and Attribute Prediction Benchmark/images/Sweet_Crochet_Blouse/img_00000070.jpg"
predict_image(model, image_path)
